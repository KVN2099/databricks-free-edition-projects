{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ac2608-a64b-4175-8e99-fc6ad48f11cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# Setup: libraries, MLflow experiment/registry, and config\n",
    "import os\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "from mlflow import sklearn as mlflow_sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Databricks feature engineering\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Config\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Unity Catalog table with Iris data\n",
    "    source_table: str = \"workspace.iris.iris_species\"\n",
    "    # Primary key present in the table (see attached schema screenshot)\n",
    "    primary_key: str = \"id\"\n",
    "    # Target column name\n",
    "    target_col: str = \"species\"\n",
    "    # Registered model name in Unity Catalog\n",
    "    registered_model: str = \"workspace.iris.models/iris_rf_classifier\"\n",
    "    # MLflow experiment path (use a workspace path so it shows in UI)\n",
    "    experiment_path: str = \"/Shared/iris_feature_store_demo\"\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# Point MLflow to the desired experiment\n",
    "mlflow.set_experiment(CFG.experiment_path)\n",
    "\n",
    "# Initialize Feature Engineering client\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "print(f\"Experiment set to: {mlflow.get_experiment_by_name(CFG.experiment_path).experiment_id}\")\n",
    "print(f\"Registered model: {CFG.registered_model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or use a Feature Engineering feature table for Iris\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CFG.registered_model.split('/')[0]}\")\n",
    "\n",
    "feature_table_name = \"workspace.iris.features/iris_measurements_ft\"\n",
    "\n",
    "# Create a managed feature table if it doesn't exist, directly from the source Unity Catalog table\n",
    "# Note: In real projects, you might build features with transformations; here we pass through numeric columns\n",
    "numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "source_df = spark.read.table(CFG.source_table).select(CFG.primary_key, *numeric_cols, CFG.target_col)\n",
    "\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "# Ensure feature table existence\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[CFG.primary_key],\n",
    "    schema=source_df.select(CFG.primary_key, *numeric_cols).schema,\n",
    "    description=\"Iris numeric measurements as features\",\n",
    "    partition_columns=None,\n",
    "    tags={\"project\": \"iris\", \"dataset\": \"workspace.iris.iris_species\"},\n",
    "    if_not_exists=True,\n",
    ")\n",
    "\n",
    "# Write features (overwrite for idempotency in demo)\n",
    "fe.write_table(\n",
    "    name=feature_table_name,\n",
    "    df=source_df.select(CFG.primary_key, *numeric_cols),\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "\n",
    "# Build a training set from the feature table; label is in the source table\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=feature_table_name,\n",
    "        lookup_key=CFG.primary_key,\n",
    "        feature_names=numeric_cols,\n",
    "    )\n",
    "]\n",
    "\n",
    "training_set = fe.create_training_set(\n",
    "    df=source_df.select(CFG.primary_key, CFG.target_col),\n",
    "    feature_lookups=feature_lookups,\n",
    "    label=CFG.target_col,\n",
    ")\n",
    "\n",
    "training_pd = training_set.load_df().toPandas()\n",
    "\n",
    "X = training_pd[numeric_cols].astype(float)\n",
    "y = training_pd[CFG.target_col].astype(str)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Shapes - train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with MLflow autolog\n",
    "import json\n",
    "\n",
    "mlflow.sklearn.autolog(log_models=True)\n",
    "run_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"max_depth\": None,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"iris_rf_training\") as run:\n",
    "    clf = RandomForestClassifier(**run_params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on validation\n",
    "    val_preds = clf.predict(X_val)\n",
    "    f1_val = f1_score(y_val, val_preds, average=\"weighted\")\n",
    "\n",
    "    mlflow.log_metric(\"f1_val\", float(f1_val))\n",
    "    mlflow.log_dict({\"numeric_features\": list(X.columns)}, \"features.json\")\n",
    "\n",
    "    # Final test metrics for reporting\n",
    "    test_preds = clf.predict(X_test)\n",
    "    f1_test = f1_score(y_test, test_preds, average=\"weighted\")\n",
    "    mlflow.log_metric(\"f1_test\", float(f1_test))\n",
    "\n",
    "    # Log the training set lineage using Feature Engineering client\n",
    "    fe.log_model(\n",
    "        model=clf,\n",
    "        artifact_path=\"model\",\n",
    "        flavor=mlflow_sklearn,\n",
    "        training_set=training_set,\n",
    "        registered_model_name=CFG.registered_model,\n",
    "    )\n",
    "\n",
    "    latest_model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    print(\"Validation F1:\", f1_val, \" Test F1:\", f1_test)\n",
    "    print(\"Model URI:\", latest_model_uri)\n",
    "\n",
    "latest_run = mlflow.last_active_run()\n",
    "latest_run_id = latest_run.info.run_id if latest_run is not None else None\n",
    "print(\"Latest Run:\", latest_run_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare latest model vs current champion by F1 and update alias if better\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_name = CFG.registered_model\n",
    "champion_alias = \"champion\"\n",
    "\n",
    "# Helper: get metric value for a model version (f1_val prioritized, fallback to f1_test)\n",
    "def _get_model_version_f1(model_name: str, version: str) -> float:\n",
    "    mv = client.get_model_version(model_name, version)\n",
    "    run = client.get_run(mv.run_id)\n",
    "    data = run.data\n",
    "    metrics = dict(data.metrics)\n",
    "    # Prefer validation F1; fallback to test\n",
    "    return float(metrics.get(\"f1_val\", metrics.get(\"f1_test\", float(\"nan\"))))\n",
    "\n",
    "# Resolve current champion version (if any)\n",
    "champion_version = None\n",
    "try:\n",
    "    aliases = client.get_model_version_by_alias(model_name, champion_alias)\n",
    "    champion_version = aliases.version\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Get latest created version\n",
    "latest_versions = client.get_latest_versions(model_name)\n",
    "latest_version = None\n",
    "if latest_versions:\n",
    "    # Choose the numerically greatest version as \"latest\"\n",
    "    latest_version = sorted(latest_versions, key=lambda mv: int(mv.version))[-1].version\n",
    "\n",
    "if latest_version is None:\n",
    "    raise RuntimeError(\"No registered versions found for model. Ensure previous cell registered the model.\")\n",
    "\n",
    "latest_f1 = _get_model_version_f1(model_name, latest_version)\n",
    "print(f\"Latest version {latest_version} F1: {latest_f1}\")\n",
    "\n",
    "champion_f1 = None\n",
    "if champion_version is not None:\n",
    "    champion_f1 = _get_model_version_f1(model_name, champion_version)\n",
    "    print(f\"Current champion version {champion_version} F1: {champion_f1}\")\n",
    "else:\n",
    "    print(\"No current champion alias set.\")\n",
    "\n",
    "should_promote = champion_f1 is None or (\n",
    "    np.isfinite(latest_f1) and np.isfinite(champion_f1) and latest_f1 > champion_f1\n",
    ")\n",
    "\n",
    "if should_promote:\n",
    "    # Point the champion alias to the latest version\n",
    "    client.set_model_version_tag(model_name, latest_version, key=\"promoted_at\", value=str(pd.Timestamp.utcnow()))\n",
    "    client.set_registered_model_alias(model_name, champion_alias, latest_version)\n",
    "    print(f\"Promoted version {latest_version} to alias '{champion_alias}'.\")\n",
    "else:\n",
    "    print(\"No promotion performed; champion F1 is greater or equal.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Model Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
