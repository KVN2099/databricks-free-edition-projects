{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2cee15-fc6e-45b0-a07c-68055a9d835a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = dbutils.widgets.get(\"CATALOG\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "VOLUME = dbutils.widgets.get(\"VOLUME\")\n",
    "TABLE = dbutils.widgets.get(\"TABLE\")\n",
    "EXPERIMENT = dbutils.widgets.get(\"EXPERIMENT\")\n",
    "\n",
    "table_path = f\"{CATALOG}.{SCHEMA}.{TABLE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04dd5c1-1675-448f-83bc-9cdf5930af30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc96007-45fa-4ad7-98c7-12413c5676d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT)\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02b9f26-6287-45c3-bbbf-c449c67142ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "BASE_DIR = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/chest-xray-pneumonia/chest_xray_aug/\"\n",
    "\n",
    "train_dir = os.path.join(BASE_DIR, \"train\")\n",
    "val_dir   = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "classes = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "n_to_move = 100\n",
    "\n",
    "for cls in classes:\n",
    "    src_dir = os.path.join(train_dir, cls)\n",
    "    dst_dir = os.path.join(val_dir, cls)\n",
    "\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    # list files in source class directory\n",
    "    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # take up to n_to_move files (handles case where there are fewer than 100)\n",
    "    to_move = files[:n_to_move]\n",
    "\n",
    "    print(f\"Moving {len(to_move)} files from {src_dir} to {dst_dir} ...\")\n",
    "\n",
    "    for fname in to_move:\n",
    "        src = os.path.join(src_dir, fname)\n",
    "        dst = os.path.join(dst_dir, fname)\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f40f8ece-4d5c-4d92-8419-18dba2280d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "###############################################################################\n",
    "# Config\n",
    "###############################################################################\n",
    "\n",
    "BATCH_SIZE = 32          # per-GPU batch size\n",
    "NUM_EPOCHS = 20\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MODEL_NAME = \"resnet18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f6f54e0-d586-4bf8-b867-3fca39341776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# DDP setup helpers\n",
    "###############################################################################\n",
    "\n",
    "def setup_ddp() -> Tuple[torch.device, int, int]:\n",
    "    \"\"\"\n",
    "    Initialize torch.distributed and return (device, rank, world_size).\n",
    "    Assumes env variables: RANK, WORLD_SIZE, LOCAL_RANK or similar\n",
    "    are set by the serverless orchestrator.\n",
    "    \"\"\"\n",
    "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    else:\n",
    "        # Fallback to single-process multi-GPU or single GPU\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "\n",
    "    if \"LOCAL_RANK\" in os.environ:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    else:\n",
    "        local_rank = 0\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(f\"cuda:{local_rank}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    if world_size > 1:\n",
    "        torch.distributed.init_process_group(\n",
    "            backend=\"nccl\" if device.type == \"cuda\" else \"gloo\",\n",
    "            rank=rank,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    return device, rank, world_size\n",
    "\n",
    "\n",
    "def cleanup_ddp():\n",
    "    if torch.distributed.is_initialized():\n",
    "        torch.distributed.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38519ea-68b6-42cf-87ed-130a98bd0948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Model\n",
    "###############################################################################\n",
    "\n",
    "def build_model(num_classes: int) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Use an ImageNet-pretrained ResNet18 and replace the final layer [web:7][web:15].\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# Training & evaluation loops\n",
    "###############################################################################\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    rank: int,\n",
    "):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if rank == 0 and step % 50 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step} Loss {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / max(total, 1)\n",
    "    epoch_acc = correct / max(total, 1)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    loss_val = running_loss / max(total, 1)\n",
    "    acc_val = correct / max(total, 1)\n",
    "    return loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec8a789-3900-451d-8c2f-fbdb5cb33cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Data\n",
    "###############################################################################\n",
    "\n",
    "def get_transforms(image_size: int = 224):\n",
    "    # Typical ImageNet-like transforms for chest X-ray classification [web:6][web:9]\n",
    "    train_tf = T.Compose([\n",
    "        T.Resize((image_size, image_size)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    eval_tf = T.Compose([\n",
    "        T.Resize((image_size, image_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    return train_tf, eval_tf\n",
    "\n",
    "\n",
    "def get_dataloaders(rank: int, world_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Expects this folder structure (Kaggle-style) [web:1][web:11][web:14]:\n",
    "        chest_xray_aug/\n",
    "          train/\n",
    "            NORMAL/\n",
    "            PNEUMONIA/\n",
    "          val/\n",
    "            NORMAL/\n",
    "            PNEUMONIA/\n",
    "          test/\n",
    "            NORMAL/\n",
    "            PNEUMONIA/\n",
    "    \"\"\"\n",
    "    train_tf, eval_tf = get_transforms()\n",
    "\n",
    "    train_dir = BASE_DIR + \"train\"\n",
    "    val_dir   = BASE_DIR + \"val\"\n",
    "    test_dir  = BASE_DIR + \"test\"\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "    val_dataset   = datasets.ImageFolder(val_dir, transform=eval_tf)\n",
    "    test_dataset  = datasets.ImageFolder(test_dir, transform=eval_tf)\n",
    "\n",
    "    if world_size > 1:\n",
    "        train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "        val_sampler   = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "        test_sampler  = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "        shuffle_flag = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        val_sampler = None\n",
    "        test_sampler = None\n",
    "        shuffle_flag = True\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=shuffle_flag if train_sampler is None else False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=val_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=test_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adb81661-f9b3-47b9-92c7-624facd322d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# MLflow-enabled main\n",
    "###############################################################################\n",
    "\n",
    "device, rank, world_size = setup_ddp()\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "  train_loader, val_loader, test_loader = get_dataloaders(rank, world_size)\n",
    "  num_classes = len(train_loader.dataset.classes)\n",
    "\n",
    "  model = build_model(num_classes)\n",
    "  model.to(device)\n",
    "\n",
    "  if world_size > 1:\n",
    "      # Wrap with DDP for multi-GPU training [web:7]\n",
    "      model = DDP(model, device_ids=[device.index], output_device=device.index)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss().to(device)\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "  best_val_acc = 0.0\n",
    "\n",
    "  for epoch in range(1, NUM_EPOCHS + 1):\n",
    "      if world_size > 1:\n",
    "          # Ensure each epoch shuffles differently in DistributedSampler\n",
    "          train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "      train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, rank)\n",
    "      val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "      # Aggregate metrics across processes if needed\n",
    "      if world_size > 1:\n",
    "          # Convert to tensors and reduce\n",
    "          metrics = torch.tensor(\n",
    "              [train_loss, train_acc, val_loss, val_acc],\n",
    "              device=device,\n",
    "          )\n",
    "          torch.distributed.all_reduce(metrics, op=torch.distributed.ReduceOp.SUM)\n",
    "          metrics = metrics / world_size\n",
    "          train_loss, train_acc, val_loss, val_acc = metrics.tolist()\n",
    "\n",
    "      if rank == 0:\n",
    "          print(\n",
    "              f\"Epoch {epoch}/{NUM_EPOCHS} \"\n",
    "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "          )\n",
    "          mlflow.log_metrics(\n",
    "              {\n",
    "                  \"train_loss\": train_loss,\n",
    "                  \"train_acc\": train_acc,\n",
    "                  \"val_loss\": val_loss,\n",
    "                  \"val_acc\": val_acc,\n",
    "              },\n",
    "              step=epoch,\n",
    "          )\n",
    "\n",
    "          # Save best model\n",
    "          if val_acc > best_val_acc:\n",
    "              best_val_acc = val_acc\n",
    "              # For DDP, model.module is the underlying model\n",
    "              model_to_log = model.module if isinstance(model, DDP) else model\n",
    "              mlflow.pytorch.log_model(model_to_log, artifact_path=\"model_best\")\n",
    "\n",
    "  # Final test evaluation on best model (reload from MLflow or just use in-memory)\n",
    "  test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "  if world_size > 1:\n",
    "      metrics = torch.tensor([test_loss, test_acc], device=device)\n",
    "      torch.distributed.all_reduce(metrics, op=torch.distributed.ReduceOp.SUM)\n",
    "      metrics = metrics / world_size\n",
    "      test_loss, test_acc = metrics.tolist()\n",
    "\n",
    "  if rank == 0:\n",
    "      print(f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}\")\n",
    "      mlflow.log_metrics({\"test_loss\": test_loss, \"test_acc\": test_acc})\n",
    "      # Log final model as well\n",
    "      model_to_log = model.module if isinstance(model, DDP) else model\n",
    "      mlflow.pytorch.log_model(model_to_log, artifact_path=\"model_final\")\n",
    "\n",
    "  cleanup_ddp()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "H100",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "databricks_ai_v4",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5921696601584323,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "4. Neural Network Training",
   "widgets": {
    "CATALOG": {
     "currentValue": "kevo_7474653918710303",
     "nuid": "b08b00bf-2991-4702-95d7-a56d8a03cf2b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "EXPERIMENT": {
     "currentValue": "/Workspace/Users/kevin.romero@databricks.com/Personal/.bundle/x-ray-classification/x-ray-experiment",
     "nuid": "057053f5-a635-47f2-a579-1505146f691e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "EXPERIMENT",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "EXPERIMENT",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "SCHEMA": {
     "currentValue": "kevin_romero",
     "nuid": "9fc8a829-0be0-4dd2-9039-983925645cc3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "SCHEMA",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "SCHEMA",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "TABLE": {
     "currentValue": "chest_xray_cleaned",
     "nuid": "f1d19783-90b5-483b-847c-eba0b676f5ff",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "TABLE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "TABLE",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "VOLUME": {
     "currentValue": "kaggle",
     "nuid": "4161a3a2-fe25-4583-8de7-1ddd4362d042",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "VOLUME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "VOLUME",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
